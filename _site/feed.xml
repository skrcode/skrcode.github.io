<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-12-11T01:35:59+05:30</updated><id>http://localhost:4000/</id><title>Suraj Rajan</title><subtitle>Software Engineer, Competitive Programmer, NLP enthusiast</subtitle><entry><title>Kaggle Spooky Author Identification - 0.29 Public LB score, Beginner NLP Tutorial</title><link href="http://localhost:4000/https:/www.kaggle.com/skrcode/0-29-public-lb-score-beginner-nlp-tutorial" rel="alternate" type="text/html" title="Kaggle Spooky Author Identification - 0.29 Public LB score, Beginner NLP Tutorial" /><published>2017-12-11T00:00:00+05:30</published><updated>2017-12-11T00:00:00+05:30</updated><id>http://localhost:4000/https:/www.kaggle.com/skrcode/kaggle_SpookyAuthor</id><content type="html" xml:base="http://localhost:4000/https:/www.kaggle.com/skrcode/0-29-public-lb-score-beginner-nlp-tutorial">&lt;p&gt;My published kernel in the Kaggle Contest, Spooky Author Identification. Highest Public LB score - 0.29 for a published kernel in the contest. Uses Simple Feature Engineering like Punctuation,Stop Words,Glove Sentence vectors. 
In addition, it creates stack features from simple Features such as tfidf and count vectors for words and chars. Multinomial naive bayes(mnb) is then applied with the following combination - tfidf+words+mnb,tfidf+chars+mnb,count+words+mnb,count+chars+mnb. 
Conv Nets on keras texttosequence, NNs on glove sentence vectors and Fast Text are also used as stack features. 
XGBoost, which is the final model which will use the simple and stack features as input.&lt;/p&gt;</content><summary>My published kernel in the Kaggle Contest, Spooky Author Identification. Highest Public LB score - 0.29 for a published kernel in the contest. Uses Simple Feature Engineering like Punctuation,Stop Words,Glove Sentence vectors. 
In addition, it creates stack features from simple Features such as tfidf and count vectors for words and chars. Multinomial naive bayes(mnb) is then applied with the following combination - tfidf+words+mnb,tfidf+chars+mnb,count+words+mnb,count+chars+mnb. 
Conv Nets on keras texttosequence, NNs on glove sentence vectors and Fast Text are also used as stack features. 
XGBoost, which is the final model which will use the simple and stack features as input.</summary></entry><entry><title>Dynamic Programming Tutorial – Level 1 (Easy-Medium)- part 2/2</title><link href="http://localhost:4000/https:/nitkcccc.wordpress.com/2014/03/20/dynamic-programming-tutorial-level-1-easy-medium-part-22/" rel="alternate" type="text/html" title="Dynamic Programming Tutorial – Level 1 (Easy-Medium)- part 2/2" /><published>2014-03-20T00:00:00+05:30</published><updated>2014-03-20T00:00:00+05:30</updated><id>http://localhost:4000/https:/nitkcccc.wordpress.com/2014/03/20/dp2</id><content type="html" xml:base="http://localhost:4000/https:/nitkcccc.wordpress.com/2014/03/20/dynamic-programming-tutorial-level-1-easy-medium-part-22/">&lt;p&gt;This tutorial explains on how memoization is relevant in reducing time complexity. In addition, it focusses on two standard problems in Dynamic Programming, the Longest Increasing Subsequence problem and the Coin Denomination problem.&lt;/p&gt;</content><summary>This tutorial explains on how memoization is relevant in reducing time complexity. In addition, it focusses on two standard problems in Dynamic Programming, the Longest Increasing Subsequence problem and the Coin Denomination problem.</summary></entry><entry><title>Dynamic Programming Tutorial – Level 1 (Easy-Medium)- part 1/2</title><link href="http://localhost:4000/https:/surajkrajan95.wordpress.com/2014/02/27/dynamic-programming-tutorial-level-1-easy-medium-part-12/" rel="alternate" type="text/html" title="Dynamic Programming Tutorial – Level 1 (Easy-Medium)- part 1/2" /><published>2014-02-27T00:00:00+05:30</published><updated>2014-02-27T00:00:00+05:30</updated><id>http://localhost:4000/https:/surajkrajan95.wordpress.com/2014/02/27/dp1</id><content type="html" xml:base="http://localhost:4000/https:/surajkrajan95.wordpress.com/2014/02/27/dynamic-programming-tutorial-level-1-easy-medium-part-12/">&lt;p&gt;This tutorial gives an intuition on why Dynamic Programming is fun to code and extremely useful in Computer Science. It talks about Dynamic Programming using a basic example, the Subset Sum problem. The tutorial further adds more intuition as to how Dynamic Programming problems should be approached along with additional practice problems.&lt;/p&gt;</content><summary>This tutorial gives an intuition on why Dynamic Programming is fun to code and extremely useful in Computer Science. It talks about Dynamic Programming using a basic example, the Subset Sum problem. The tutorial further adds more intuition as to how Dynamic Programming problems should be approached along with additional practice problems.</summary></entry></feed>
